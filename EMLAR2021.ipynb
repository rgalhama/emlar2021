{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantic Models in Python\n",
    "## EMLAR 2021\n",
    "\n",
    "### Raquel G. Alhama, Tilburg University\n",
    "### Andrew Jessop, University of Liverpool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import some libraries that will be useful\n",
    "#You may uncomment the following lines the first time you run the code\n",
    "#!conda install --yes  numpy nltk \n",
    "#!conda install --yes  -c conda-forge scikit-learn \n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "\n",
    "You can try this code with any textual data. For this example, we will use some children's books from Gutenberg project.\n",
    "https://www.gutenberg.org/ebooks/bookshelves/search/?query=children|child|school\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from book that we have previously downloaded from Gutenberg project\n",
    "raw = open(\"pg6328.txt\", 'r').read()\n",
    "\n",
    "#The variable \"raw\" contains now all the text from this file. \n",
    "\n",
    "#Let's have a look at the data:\n",
    "#print(raw[4000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At the moment, what raw contains is a string of characters. But we are interested in words rather than characters.\n",
    "\n",
    "#How can we separate this text into sentences?\n",
    "\n",
    "#This process is called \"tokenization\". \n",
    "\n",
    "#First we need to download a tokenizer from NLTK:\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#Now let's tokenize each sentence.\n",
    "#nltk.sent_tokenize gives us a method to do so\n",
    "sentences = nltk.sent_tokenize(raw)\n",
    "\n",
    "#And we tokenize all the words in each sentence and collect them together \n",
    "tokenized = []\n",
    "header = True\n",
    "for sentence in sentences:\n",
    "    if header and sentence.startswith(\"CONTENTS\"):\n",
    "        header = False\n",
    "    if not header: #We ignore everything before the table of contents\n",
    "        tokenized.append(nltk.wordpunct_tokenize(sentence))\n",
    "    \n",
    "\n",
    "#Let's look at the words. Do you spot any problem? \n",
    "#print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's lowercase the text:\n",
    "lowercased = []\n",
    "for sentence in tokenized:\n",
    "    lowercased.append( [s.lower() for s in sentence] )\n",
    "        \n",
    "#print(lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are going to use some *regular expressions* via the [re](https://docs.python.org/3/library/re.html) package. Regular expressions (often shortened to *regex*) is a useful system for finding patterns in text. It is useful for preparing data for in modelling as it helps with unwanted characters (like punctuation) or searching text.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's remove the all the punctuation\n",
    "wordsonly = []\n",
    "for sentence in lowercased:\n",
    "    words = []\n",
    "    for s in sentence:\n",
    "        word = re.sub(r'[^a-z]', '', s)\n",
    "        if word != '': # We don't want to add empty strings \n",
    "            words.append(word)\n",
    "    wordsonly.append(words)\n",
    "#print(wordsonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's count the words! \n",
    "\n",
    "#We first build a dictionary with word types and their frequencies\n",
    "word_frequencies = {}\n",
    "for sentence in wordsonly:\n",
    "    for s in sentence:\n",
    "        word_frequencies[s] = word_frequencies.get(s, 0) + 1\n",
    "\n",
    "#print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at some of the most frequent words.\n",
    "# We construct a list of words ordered from most frequent to most infrequent\n",
    "sorted_keys = sorted(word_frequencies, key = word_frequencies.get, reverse = True)\n",
    "print(\"These are the 10 most frequent words: \", sorted_keys[:10])\n",
    "print(\"These are the 10 most infrequent words: \", sorted_keys[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is generally better to restrict models to words with a minimum frequency.\n",
    "# We define a minimum frequency threshold of 10 and filter the words:\n",
    "minfreq = 10\n",
    "target_freqs = dict([(word,freq) for word,freq in word_frequencies.items() if freq > minfreq])\n",
    "#Now target_freqs is a dictionary with all the words we are interested in (we call them targets), and their frequency.\n",
    "\n",
    "#It will be useful to have also the list of targets:\n",
    "targets = target_freqs.keys()\n",
    "#And the vocabulary size\n",
    "vocabulary_size=len(target_freqs)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home exercise: \n",
    "remove also the most frequent words (\"stopwords\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will be useful to have a numerical index for each word\n",
    "# We will use it later use to locate the word in the co-occurrence matrix\n",
    "# Word to index:\n",
    "w2i = {w: i for i, w in enumerate(targets)}\n",
    "# Index to word:\n",
    "i2w = {i: w for i, w in enumerate(targets)}\n",
    "\n",
    "#Example:\n",
    "print(\"The code for the word \\\"cave\\\" is {}\".format(w2i[\"cave\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Distributional Semantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we write the functions that we use to build the Distributional Semantic Model.\n",
    "\n",
    "# This function is used to build the co-occurrence matrix\n",
    "def calculate_cooccurrences(tokenized, vocabulary_size, window):\n",
    "    matrix = np.zeros([vocabulary_size, vocabulary_size]) \n",
    "    for sentence in tokenized:\n",
    "        for position,word in enumerate(sentence):                \n",
    "            for j in range(max(position-window,0),min(position+window+1,len(sentence))):\n",
    "                context=sentence[j]\n",
    "                if j!=position and word in targets and context in targets: \n",
    "                    matrix[w2i[word]][w2i[context]]+=1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "#This function will give us the co-occurrence counts between two words, given a co-occurrence matrix\n",
    "def get_cooccurrence(word1, word2, counts):\n",
    "    return counts[w2i[word1]][w2i[word2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now compute the co-occurrences in our tokenized text\n",
    "count_matrix = calculate_cooccurrences(wordsonly, vocabulary_size, 2)\n",
    "\n",
    "#Let's have a look at some co-occurrences:\n",
    "print(get_cooccurrence(\"horse\", \"house\", count_matrix))\n",
    "print(get_cooccurrence(\"next\", \"morning\", count_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can apply any transformation to this matrix of counts (e.g. Pointwise Mutual Information). We leave this as an exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances (semantic similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can compute a matrix with the cosine distance between every word:\n",
    "similarities = cosine_similarity(count_matrix)\n",
    "#print(similarities)\n",
    "\n",
    "#This function will give us the co-occurrence between two words\n",
    "def get_similarities(word1, word2, similarities):\n",
    "    return similarities[w2i[word1]][w2i[word2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the similarities between these words\n",
    "print(get_similarities(\"eat\", \"drink\", similarities))\n",
    "print(get_similarities(\"lamp\",\"door\", similarities))\n",
    "print(get_similarities(\"lamp\",\"drink\", similarities))\n",
    "\n",
    "#Try other examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to compute the similarities, you can do any type of analyses.\n",
    "For example, correlate the similarities of the model with human similarity judgements,  find the closest neighbours to one word, compare similarities between models of books from different periods, etcs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "The file thomas.txt contains child-directed speech from the Thomas corpus in CHILDES.\n",
    "\n",
    "Re-use the previous code to build a Distributional Semantic Model for this data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
